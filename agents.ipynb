{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import os\n",
    "gpt4o = dspy.LM('openai/gpt-4o', api_key=os.getenv('OPENAI_API_KEY'), temperature=0.7)\n",
    "dspy.configure(lm=gpt4o)\n",
    "print(os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 9.21M/9.21M [00:00<00:00, 18.4MB/s]\n",
      "Downloading data: 100%|██████████| 2.15M/2.15M [00:00<00:00, 12.3MB/s]\n",
      "Downloading data: 100%|██████████| 899k/899k [00:00<00:00, 21.7MB/s]\n",
      "Generating train split: 100%|██████████| 18171/18171 [00:00<00:00, 23248.37 examples/s]\n",
      "Generating validation split: 100%|██████████| 4000/4000 [00:00<00:00, 28531.03 examples/s]\n",
      "Generating test split: 100%|██████████| 4000/4000 [00:00<00:00, 44299.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "kwargs = dict(fields=(\"claim\", \"supporting_facts\", \"hpqa_id\", \"num_hops\"), input_keys=(\"claim\",))\n",
    "hover = DataLoader().from_huggingface(dataset_name=\"hover-nlp/hover\", split=\"train\", trust_remote_code=True, **kwargs)\n",
    "\n",
    "hpqa_ids = set()\n",
    "hover = [\n",
    "    dspy.Example(claim=x.claim, titles=list(set([y[\"key\"] for y in x.supporting_facts]))).with_inputs(\"claim\")\n",
    "    for x in hover\n",
    "    if x[\"num_hops\"] == 3 and x[\"hpqa_id\"] not in hpqa_ids and not hpqa_ids.add(x[\"hpqa_id\"])\n",
    "]\n",
    "\n",
    "random.Random(0).shuffle(hover)\n",
    "trainset, devset, testset = hover[:100], hover[100:200], hover[650:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the datasets to pickle files\n",
    "with open('trainset.txt', 'wb') as f:\n",
    "    pickle.dump(trainset, f)\n",
    "\n",
    "with open('devset.txt', 'wb') as f:\n",
    "    pickle.dump(devset, f)\n",
    "\n",
    "with open('testset.txt', 'wb') as f:\n",
    "    pickle.dump(testset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: This director is known for his work on Miss Potter. The Academy of Motion Picture Arts and Sciences presents the award in which he was nominated for his work in \"Babe\".\n",
      "Pages that must be retrieved: ['Chris Noonan', 'Miss Potter', 'Academy Award for Best Director']\n"
     ]
    }
   ],
   "source": [
    "example = trainset[0]\n",
    "\n",
    "print(\"Claim:\", example.claim)\n",
    "print(\"Pages that must be retrieved:\", example.titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = {}\n",
    "\n",
    "def search(query: str, k: int) -> list[str]:\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=k)\n",
    "    results = [x['text'] for x in results]\n",
    "\n",
    "    for result in results:\n",
    "        title, text = result.split(\" | \", 1)\n",
    "        DOCS[title] = text\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikipedia(query: str) -> list[str]:\n",
    "    \"\"\"Returns top-5 results and then the titles of the top-5 to top-30 results.\"\"\"\n",
    "\n",
    "    topK = search(query, 30)\n",
    "    titles, topK = [f\"`{x.split(' | ')[0]}`\" for x in topK[5:30]], topK[:5]\n",
    "    return topK + [f\"Other retrieved pages have titles: {', '.join(titles)}.\"]\n",
    "\n",
    "def lookup_wikipedia(title: str) -> str:\n",
    "    \"\"\"Returns the text of the Wikipedia page, if it exists.\"\"\"\n",
    "\n",
    "    if title in DOCS:\n",
    "        return DOCS[title]\n",
    "\n",
    "    results = [x for x in search(title, 10) if x.startswith(title + \" | \")]\n",
    "    if not results:\n",
    "        return f\"No Wikipedia page found for title: {title}\"\n",
    "    return results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Find all Wikipedia titles relevant to verifying (or refuting) the claim.\"\n",
    "signature = dspy.Signature(\"claim -> titles: list[str]\", instructions)\n",
    "react = dspy.ReAct(signature, tools=[search_wikipedia, lookup_wikipedia], max_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David Gregory (physician)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react(claim=\"David Gregory was born in 1625.\").titles[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5_recall(example, pred, trace=None):\n",
    "    gold_titles = example.titles\n",
    "    recall = sum(x in pred.titles[:5] for x in gold_titles) / len(gold_titles)\n",
    "\n",
    "    # If we're \"bootstrapping\" for optimization, return True if and only if the recall is perfect.\n",
    "    if trace is not None:\n",
    "        return recall >= 1.0\n",
    "    \n",
    "    # If we're just doing inference, just measure the recall.\n",
    "    return recall\n",
    "\n",
    "evaluate = dspy.Evaluate(devset=devset, metric=top5_recall, num_threads=16, display_progress=True, display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 73 (4.1%):  73%|███████▎  | 73/100 [03:45<01:27,  3.25s/it] "
     ]
    }
   ],
   "source": [
    "def safe_react(claim: str):\n",
    "    try:\n",
    "        return react(claim=claim)\n",
    "    except Exception as e:\n",
    "        return dspy.Prediction(titles=[])\n",
    "\n",
    "evaluate(safe_react)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
